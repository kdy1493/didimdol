import torch
import numpy as np
import os
from glob import glob
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import KFold
from stgcn_graph import STGCNGraphConv  # STGCNGraphConv ë¶ˆëŸ¬ì˜¤ê¸°
from convert_stGCN import ACTIVITY_TO_LABEL  # í–‰ë™ í´ë˜ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬

# === CUDA ì„¤ì • ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ’» Using device: {device}")

# === ë°ì´í„° í´ë” ì„¤ì • ===
skeleton_folder = "./skeleton_data"
labels_folder = "./frame_labels"

# === íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ===
skeleton_files = sorted(glob(os.path.join(skeleton_folder, "*.npy")))
label_files = sorted(glob(os.path.join(labels_folder, "*.npy")))

# === PyTorch Dataset ì •ì˜ ===
class SkeletonDataset(Dataset):
    def __init__(self, skeleton_files, label_files):
        self.skeleton_files = skeleton_files
        self.label_files = label_files

    def __len__(self):
        return len(self.skeleton_files)

    def __getitem__(self, idx):
        # NumPy ë°ì´í„° ë¡œë“œ
        skeleton_data = np.load(self.skeleton_files[idx], allow_pickle=True)  # (1, 2, Frames, Nodes)
        frame_labels = np.load(self.label_files[idx], allow_pickle=True)  # (Frames,)

        # PyTorch Tensor ë³€í™˜ (GPUë¡œ ì´ë™)
        skeleton_tensor = torch.tensor(skeleton_data, dtype=torch.float32).to(device)
        labels_tensor = torch.tensor(frame_labels, dtype=torch.long).unsqueeze(0).to(device)  # (1, Frames)

        return skeleton_tensor, labels_tensor

# === K-Fold Cross Validation ì„¤ì • ===
num_folds = 5
num_epochs = 200
batch_size = 16
patience = 10  # Early Stopping ê¸°ì¤€ (10 Epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# === Dataset ë¡œë“œ ===
dataset = SkeletonDataset(skeleton_files, label_files)

# === Cross Validation ì‹¤í–‰ ===
test_losses = []
test_accuracies = []

for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):
    print(f"\nğŸš€ Fold [{fold+1}/{num_folds}] ì‹œì‘!")

    # Train / Validation Subset ìƒì„±
    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx)

    # DataLoader ì„¤ì •
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    # === STGCNGraphConv ëª¨ë¸ ì´ˆê¸°í™” (GPUë¡œ ì´ë™) ===
    num_classes = len(ACTIVITY_TO_LABEL)  # í–‰ë™ í´ë˜ìŠ¤ ê°œìˆ˜
    args = {
        "Kt": 3, "Ks": 3, "n_his": 10, "act_func": "relu", 
        "graph_conv_type": "gcn", "gso": None, "enable_bias": True, "droprate": 0.3
    }
    blocks = [[2, 64], [64, 128], [128, 256], [256, num_classes]]  # STGCN ë¸”ë¡ êµ¬ì¡°
    model = STGCNGraphConv(args, blocks, n_vertex=17).to(device)

    # í•™ìŠµ ì„¤ì •
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = torch.nn.CrossEntropyLoss()

    # Early Stopping ì„¤ì •
    best_val_loss = float("inf")
    patience_counter = 0

    # === í•™ìŠµ ë£¨í”„ ===
    for epoch in range(num_epochs):
        total_train_loss = 0
        total_val_loss = 0

        # === Training ===
        model.train()
        for batch_skeleton, batch_labels in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_skeleton)  # (Batch, Classes, Frames)
            
            loss = criterion(outputs.squeeze(), batch_labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

        # === Validation ===
        model.eval()
        with torch.no_grad():
            for batch_skeleton, batch_labels in val_loader:
                outputs = model(batch_skeleton)
                loss = criterion(outputs.squeeze(), batch_labels)
                total_val_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        avg_val_loss = total_val_loss / len(val_loader)

        print(f"Fold [{fold+1}/{num_folds}], Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # === Early Stopping ì²´í¬ ===
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
        else:
            patience_counter += 1  # ê°œì„ ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì¹´ìš´íŠ¸ ì¦ê°€

        if patience_counter >= patience:
            print(f"â¹ Early Stopping! Epoch [{epoch+1}/{num_epochs}]ì—ì„œ ì¢…ë£Œ (Val Loss ê°œì„  ì—†ìŒ)")
            break

    print(f"âœ… Fold [{fold+1}/{num_folds}] ì™„ë£Œ!")

    # === Test Setì—ì„œ ìµœì¢… ì„±ëŠ¥ í‰ê°€ ===
    test_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
    test_loss = 0
    correct_preds = 0
    total_preds = 0

    model.eval()
    with torch.no_grad():
        for batch_skeleton, batch_labels in test_loader:
            outputs = model(batch_skeleton)
            loss = criterion(outputs.squeeze(), batch_labels)
            test_loss += loss.item()

            # Accuracy ê³„ì‚° (ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë¼ë²¨ ë¹„êµ)
            predicted = torch.argmax(outputs, dim=1)
            correct_preds += (predicted == batch_labels).sum().item()
            total_preds += batch_labels.numel()

    avg_test_loss = test_loss / len(test_loader)
    test_accuracy = correct_preds / total_preds

    test_losses.append(avg_test_loss)
    test_accuracies.append(test_accuracy)

    print(f"ğŸ“Š Fold [{fold+1}/{num_folds}] Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# === Cross Validation ê²°ê³¼ ì¶œë ¥ ===
print("\nğŸ¯ ìµœì¢… Test ê²°ê³¼:")
print(f"ğŸ“‰ í‰ê·  Test Loss: {np.mean(test_losses):.4f}")
print(f"âœ… í‰ê·  Test Accuracy: {np.mean(test_accuracies):.4f}")
